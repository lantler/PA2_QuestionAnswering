{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d456e1f9-484f-4943-af6d-00f9fe69e58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from readability import Document\n",
    "\n",
    "\n",
    "#for NER\n",
    "import en_core_web_sm\n",
    "nlp= en_core_web_sm.load()\n",
    "import re\n",
    "import sys\n",
    "import nltk\n",
    "import wikipediaapi\n",
    "from nltk.tokenize import sent_tokenize, RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0c8d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the necessary NLTK resources are available\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6de14a00-05c2-4e4d-b569-3a83d56c98c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Define patterns\n",
    "who_pat = r\"\\b[Ww]ho\\b\"\n",
    "where_pat = r\"\\b[Ww]here\\b\"\n",
    "what_pat = r\"\\b[Ww]hat\\b\"\n",
    "when_pat = r\"\\b[Ww]hen\\b\"\n",
    "\n",
    "# Initialize the spaCy model\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize global variables\n",
    "query = \"\"\n",
    "entity = \"\"\n",
    "\n",
    "#classify the query so we can know what the user wants answered\n",
    "def classify_query(query):\n",
    "    query_toks = tokenizer.tokenize(query)\n",
    "    \n",
    "    query_type = \"\"\n",
    "    for word in query_toks:\n",
    "        if re.search(who_pat, word):\n",
    "            query_type = \"Person\"\n",
    "            print(\"Person Query\")\n",
    "        elif re.search(where_pat, word):\n",
    "            query_type = \"Location\"\n",
    "            print(\"Location Query\")\n",
    "        elif re.search(what_pat, word):\n",
    "            query_type = \"Definition\"\n",
    "            print(\"Definition Query\")\n",
    "        elif re.search(when_pat, word):\n",
    "            query_type = \"Time\"\n",
    "            print(\"When Query\")\n",
    "        \n",
    "    return query_toks, query_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2af72f40-1339-4dca-bbee-a191513709fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When did the Berlin Wall fall\n"
     ]
    }
   ],
   "source": [
    "#in the case of a named Entity, this fill find it\n",
    "\n",
    "def entity_search(tokenized_query):\n",
    "    global entity\n",
    "    # Join the tokens into a sentence\n",
    "    sent = \" \".join(tokenized_query)\n",
    "    # Perform NER on the sentence\n",
    "    doc = nlp(sent)\n",
    "    # Get the named entities\n",
    "    if doc.ents:\n",
    "        # Assign the first named entity to the global variable entity\n",
    "        entity = doc.ents[0].text\n",
    "    else:\n",
    "        # If no named entities are found, set entity to an empty string or other default value\n",
    "        entity = \"\"\n",
    "    \n",
    "    print(f\"Entity: {entity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87f1d3a3-3354-4708-8091-d65e0784f404",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in the case there isn't a named entity, we need to single out the key words (nouns/verbs)\n",
    "def find_key_words(query):\n",
    "    unimportant_words = r\"\\b(([Ww]here|[Ww]hat|[Ww]ho|[Ww]hen) (is|was|did))( (a|the))?\\b\"\n",
    "    key_words = re.sub(unimportant_words, \"\", query)\n",
    "    \n",
    "    print(f\"Key Words: {key_words}\")\n",
    "    return key_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a10a1a3-c5ea-4706-8f7f-46f97b396a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When Query\n"
     ]
    }
   ],
   "source": [
    "#sometimes the POS tags were wrong. This will try to correct it.\n",
    "\n",
    "def correct_tags(key_words):\n",
    "    corrected_tags = []\n",
    "    pos_tokens = nltk.word_tokenize(key_words)\n",
    "    tags = nltk.pos_tag(pos_tokens)\n",
    "\n",
    "    # Rule-based disambiguation\n",
    "    for i, (word, tag) in enumerate(tags):\n",
    "        if word.lower() == \"fall\" and i > 0 and tags[i-1][1] in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "            corrected_tags.append((word, 'VB'))\n",
    "        elif word.lower() == \"die\" and i > 0 and tags[i-1][1] in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "            corrected_tags.append((word, 'VB'))\n",
    "        elif word.lower() == \"born\" and i > 0 and tags[i-1][1] in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "            corrected_tags.append((word, 'VB'))\n",
    "        elif word.lower() == \"start\" and i > 0 and tags[i-1][1] in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "            corrected_tags.append((word, 'VB'))\n",
    "        else:\n",
    "            corrected_tags.append((word, tag))\n",
    "            \n",
    "    print(f\"Corrected Tags: {corrected_tags}\")\n",
    "    return corrected_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa7d1c04-81d2-4e30-b568-b79e1cdec656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'did', 'the', 'Berlin', 'Wall', 'fall']\n"
     ]
    }
   ],
   "source": [
    "#Get the nouns from the POS tags\n",
    "\n",
    "def find_noun(corrected_tags):\n",
    "    noun_tags = ['NN', 'NNS', 'NNP', 'NNPS', 'JJ', 'IN']\n",
    "\n",
    "    # Filter out only the nouns\n",
    "    nouns = [word for word, pos in corrected_tags if pos in noun_tags]\n",
    "\n",
    "    noun = \" \".join(nouns)\n",
    "    \n",
    "    print(f\"Noun: {noun}\")\n",
    "    return noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35b12d7f-d0d5-4de0-b2ec-5a6624f71605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the Berlin Wall\n"
     ]
    }
   ],
   "source": [
    "#Leahs scraping code from here\n",
    "def fetch_wikipedia_summary(topic):\n",
    "    user_agent = 'PA2/1.0 (lantler@gmu.edu)'\n",
    "    wiki_api = wikipediaapi.Wikipedia('en', headers={'User-Agent': user_agent})\n",
    "    page = wiki_api.page(topic)\n",
    "    if page.exists():\n",
    "        return page.summary\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5bf40463-bfe5-4007-b45e-5efc48e17081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(question, summary):\n",
    "    question_words = question.lower().split()\n",
    "    subject = \" \".join(question_words[2:]).replace('?', '')\n",
    "\n",
    "    sentences = sent_tokenize(summary)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if subject.lower() in sentence.lower():\n",
    "            return sentence.strip()\n",
    "\n",
    "    return \"I'm sorry, I don't know the answer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5d688bb-6ac7-4af5-9aa3-6c584192ba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global query, entity\n",
    "\n",
    "    while query.lower() != 'exit':\n",
    "        tokens, query_type = classify_query(query)\n",
    "        entity_search(tokens)\n",
    "        key_words = find_key_words(query)\n",
    "        corrected_tags = correct_tags(key_words)\n",
    "        noun = find_noun(corrected_tags)\n",
    "        \n",
    "        search_term = \"\".join(noun)\n",
    "        content_summary = fetch_wikipedia_summary(search_term)\n",
    "        print(content_summary)\n",
    "        \n",
    "        if entity:\n",
    "            answer = generate_response(entity, content_summary)\n",
    "        else:\n",
    "            answer = generate_response(search_term, content_summary)\n",
    "        \n",
    "        print(answer)\n",
    "        \n",
    "        query = input(\"Please ask a question. Type 'exit' to exit: \")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = input(\"Please ask a question, Type 'Exit' to exit: \").strip()\n",
    "    pattern = r'[^\\w\\s]'\n",
    "    query = re.sub(pattern, '', query)\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29afbeb-a1f6-4c59-8314-68dd9a6c4351",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All that's left to do is parse through the content summary and find relavent information for more complicated questions.\n",
    "#having the query type and associated verb will help to specify what the user wants answered"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
